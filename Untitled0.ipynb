{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v0ttF7cMDWxj",
        "outputId": "b014093b-5a61-4489-828f-5fb82eb8e54e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting pandas==2.2.2\n",
            "  Downloading pandas-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas==2.2.2)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas==2.2.2)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas==2.2.2)\n",
            "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas==2.2.2)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "Downloading pandas-2.2.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.7/12.7 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m347.8/347.8 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pytz, tzdata, six, numpy, python-dateutil, pandas\n",
            "  Attempting uninstall: pytz\n",
            "    Found existing installation: pytz 2025.2\n",
            "    Uninstalling pytz-2025.2:\n",
            "      Successfully uninstalled pytz-2025.2\n",
            "  Attempting uninstall: tzdata\n",
            "    Found existing installation: tzdata 2025.2\n",
            "    Uninstalling tzdata-2025.2:\n",
            "      Successfully uninstalled tzdata-2025.2\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.17.0\n",
            "    Uninstalling six-1.17.0:\n",
            "      Successfully uninstalled six-1.17.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gradio 5.43.1 requires huggingface-hub<1.0,>=0.33.5, but you have huggingface-hub 0.24.6 which is incompatible.\n",
            "diffusers 0.35.1 requires huggingface-hub>=0.34.0, but you have huggingface-hub 0.24.6 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "peft 0.17.1 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.24.6 which is incompatible.\n",
            "google-adk 1.12.0 requires watchdog<7.0.0,>=6.0.0, but you have watchdog 4.0.2 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4 pandas-2.2.2 python-dateutil-2.9.0.post0 pytz-2025.2 six-1.17.0 tzdata-2025.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil",
                  "numpy",
                  "pytz",
                  "six"
                ]
              },
              "id": "8786b7b1594244ceaf0ee4fa3310dbf9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip -q install streamlit==1.37.1 pyngrok==7.2.3 \\\n",
        "  transformers==4.43.3 accelerate==0.33.0 bitsandbytes==0.43.3 \\\n",
        "  huggingface_hub==0.24.6 sentencepiece pandas==2.2.2 matplotlib==3.9.0\n",
        "!pip install --upgrade --force-reinstall numpy==1.26.4 pandas==2.2.2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, getpass\n",
        "\n",
        "# ↓ paste your tokens when prompted\n",
        "if \"HUGGINGFACEHUB_API_TOKEN\" not in os.environ:\n",
        "    os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = getpass.getpass(\"Paste your Hugging Face token: \")\n",
        "\n",
        "if \"NGROK_AUTHTOKEN\" not in os.environ:\n",
        "    os.environ[\"NGROK_AUTHTOKEN\"] = getpass.getpass(\"Paste your ngrok authtoken: \")\n",
        "\n",
        "print(\"✅ Tokens captured in environment.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOYHHD5rDspV",
        "outputId": "2c3e2818-ccd7-4059-8c7f-9fd0653e5815"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paste your Hugging Face token: ··········\n",
            "Paste your ngrok authtoken: ··········\n",
            "✅ Tokens captured in environment.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create sample transactions DataFrame\n",
        "sample = pd.DataFrame({\n",
        "    \"date\": [\n",
        "        \"2025-08-01\",\"2025-08-02\",\"2025-08-03\",\"2025-08-04\",\"2025-08-05\",\n",
        "        \"2025-08-06\",\"2025-08-07\",\"2025-08-08\",\"2025-08-09\",\"2025-08-10\"\n",
        "    ],\n",
        "    \"description\": [\n",
        "        \"Groceries\",\"Restaurant\",\"Metro\",\"Rent\",\"Utilities\",\n",
        "        \"Cinema\",\"Mobile Plan\",\"Amazon\",\"Pharmacy\",\"Salary\"\n",
        "    ],\n",
        "    \"category\": [\n",
        "        \"Groceries\",\"Dining\",\"Transport\",\"Housing\",\"Utilities\",\n",
        "        \"Entertainment\",\"Utilities\",\"Shopping\",\"Health\",\"Income\"\n",
        "    ],\n",
        "    \"amount\": [\n",
        "        2200, 650, 120, 12000, 1800,\n",
        "        300, 499, 999, 250, -35000   # Salary as negative → income\n",
        "    ]\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "sample.to_csv(\"sample_transactions.csv\", index=False)\n",
        "\n",
        "# Confirm save\n",
        "print(\"✅ Saved sample_transactions.csv – you can upload this in the app.\")\n",
        "\n",
        "# Display DataFrame in Colab\n",
        "display(sample)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "NugeinkKDsun",
        "outputId": "781c2441-dca9-446b-c761-d6d0109f7c43"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1544982614.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create sample transactions DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m sample = pd.DataFrame({\n\u001b[1;32m      5\u001b[0m     \"date\": [\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "from typing import List, Dict, Optional\n",
        "\n",
        "# ----------------------------\n",
        "# CONFIG\n",
        "# ----------------------------\n",
        "# Choose model + running mode:\n",
        "MODEL_ID = \"ibm-granite/granite-3.3-2b-instruct\"  # good size for Colab demos\n",
        "RUN_MODE = st.session_state.get(\"RUN_MODE\", \"hosted\")  # \"hosted\" or \"local\"\n",
        "\n",
        "# Generation defaults\n",
        "GEN_KW = dict(max_new_tokens=400, temperature=0.3, top_p=0.9, repetition_penalty=1.05)\n",
        "\n",
        "# Safety disclaimer\n",
        "DISCLAIMER = \"This chatbot provides educational financial information, not professional financial advice.\"\n",
        "\n",
        "# ----------------------------\n",
        "# HELPER: Budget analysis (simple, on-device)\n",
        "# ----------------------------\n",
        "def summarize_budget(df: pd.DataFrame) -> Dict:\n",
        "    # Expect columns: date, description, category, amount (Income is negative in sample)\n",
        "    # Normalize columns\n",
        "    cols = {c.lower(): c for c in df.columns}\n",
        "    for key in [\"date\",\"description\",\"category\",\"amount\"]:\n",
        "        if key not in cols:\n",
        "            raise ValueError(f\"Missing required column: {key}\")\n",
        "    df = df.rename(columns={cols[\"date\"]:\"date\", cols[\"description\"]:\"description\",\n",
        "                            cols[\"category\"]:\"category\", cols[\"amount\"]:\"amount\"})\n",
        "    df[\"amount\"] = pd.to_numeric(df[\"amount\"], errors=\"coerce\").fillna(0.0)\n",
        "\n",
        "    by_cat = df.groupby(\"category\", dropna=False)[\"amount\"].sum().sort_values(ascending=False)\n",
        "    total_outflow = df.loc[df[\"amount\"]>0, \"amount\"].sum()\n",
        "    total_inflow  = -df.loc[df[\"amount\"]<0, \"amount\"].sum()\n",
        "    savings_est   = max(total_inflow - total_outflow, 0)\n",
        "\n",
        "    top_spends = by_cat.head(5).to_dict()\n",
        "\n",
        "    # simple insights\n",
        "    insights = []\n",
        "    if total_inflow > 0:\n",
        "        savings_rate = 100 * savings_est / total_inflow\n",
        "        insights.append(f\"Estimated savings rate: {savings_rate:.1f}%\")\n",
        "        if savings_rate < 20:\n",
        "            insights.append(\"Savings rate is below 20%. Consider trimming top discretionary categories by 10–15%.\")\n",
        "        elif savings_rate > 40:\n",
        "            insights.append(\"Strong savings rate. You could automate transfers to high-interest savings or SIPs.\")\n",
        "\n",
        "    if \"Dining\" in by_cat.index and by_cat[\"Dining\"] > 0.15 * total_outflow:\n",
        "        insights.append(\"Dining is >15% of outflow. Try meal planning or a weekly dining cap.\")\n",
        "\n",
        "    return {\n",
        "        \"total_inflow\": float(total_inflow),\n",
        "        \"total_outflow\": float(total_outflow),\n",
        "        \"estimated_savings\": float(savings_est),\n",
        "        \"top_spends\": top_spends,\n",
        "        \"insights\": insights\n",
        "    }\n",
        "\n",
        "# ----------------------------\n",
        "# LLM: two backends (hosted/local)\n",
        "# ----------------------------\n",
        "def build_system_prompt(user_type: str) -> str:\n",
        "    if user_type == \"Student\":\n",
        "        return (\"You are a friendly finance tutor for a student. \"\n",
        "                \"Use simple language, short steps, and examples in INR. \"\n",
        "                \"Always add 2–3 concrete, low-effort tips.\")\n",
        "    else:\n",
        "        return (\"You are a practical personal-finance advisor for a working professional in India. \"\n",
        "                \"Be concise, structured, and action-oriented. Assume INR. \"\n",
        "                \"Add specific, implementable next steps and simple rules of thumb.\")\n",
        "\n",
        "def craft_messages(system_prompt: str, user_prompt: str, budget_summary: Optional[Dict]) -> List[Dict]:\n",
        "    context = \"\"\n",
        "    if budget_summary:\n",
        "        context = (\n",
        "            f\"Context — totals (INR): inflow={budget_summary['total_inflow']:.0f}, \"\n",
        "            f\"outflow={budget_summary['total_outflow']:.0f}, est_savings={budget_summary['estimated_savings']:.0f}. \"\n",
        "            f\"Top categories: {json.dumps(budget_summary['top_spends'])}. \"\n",
        "            f\"Insights: {budget_summary['insights']}.\"\n",
        "        )\n",
        "    return [\n",
        "        {\"role\":\"system\",\"content\": system_prompt + \" \" + DISCLAIMER},\n",
        "        {\"role\":\"user\",\"content\": (context + \"\\n\\nUser question:\\n\" + user_prompt).strip()}\n",
        "    ]\n",
        "\n",
        "# ---- Hosted via Hugging Face Inference API ----\n",
        "def generate_hosted(messages: List[Dict], params: Dict) -> str:\n",
        "    import requests\n",
        "    token = os.environ.get(\"HUGGINGFACEHUB_API_TOKEN\", \"\")\n",
        "    if not token:\n",
        "        return \"Missing Hugging Face token. Please set HUGGINGFACEHUB_API_TOKEN.\"\n",
        "    url = f\"https://api-inference.huggingface.co/models/{MODEL_ID}\"\n",
        "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
        "    # Use chat template by joining messages; many Granite models support chat templates.\n",
        "    joined = \"\\n\".join([f\"{m['role'].upper()}: {m['content']}\" for m in messages]) + \"\\nASSISTANT:\"\n",
        "    payload = {\"inputs\": joined, \"parameters\": params}\n",
        "    r = requests.post(url, headers=headers, json=payload, timeout=90)\n",
        "    if r.status_code != 200:\n",
        "        return f\"Inference API error: {r.status_code} — {r.text[:300]}\"\n",
        "    try:\n",
        "        # API may return list or dict depending on backend\n",
        "        data = r.json()\n",
        "        if isinstance(data, list) and data and \"generated_text\" in data[0]:\n",
        "            return data[0][\"generated_text\"].split(\"ASSISTANT:\", 1)[-1].strip()\n",
        "        elif isinstance(data, dict) and \"generated_text\" in data:\n",
        "            return data[\"generated_text\"]\n",
        "        else:\n",
        "            return str(data)[:1000]\n",
        "    except Exception as e:\n",
        "        return f\"Parse error: {e}\"\n",
        "\n",
        "# ---- Local 4-bit using Transformers + bitsandbytes ----\n",
        "_local_state = {\"loaded\": False, \"model\": None, \"tokenizer\": None}\n",
        "def load_local_model():\n",
        "    if _local_state[\"loaded\"]:\n",
        "        return\n",
        "    import torch\n",
        "    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "    bnb = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_use_double_quant=True,\n",
        "                             bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16)\n",
        "    tok = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "    mdl = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        quantization_config=bnb\n",
        "    )\n",
        "    _local_state.update({\"loaded\": True, \"model\": mdl, \"tokenizer\": tok})\n",
        "\n",
        "def generate_local(messages: List[Dict], params: Dict) -> str:\n",
        "    import torch\n",
        "    from transformers import GenerationConfig\n",
        "    load_local_model()\n",
        "    tok = _local_state[\"tokenizer\"]; mdl = _local_state[\"model\"]\n",
        "    # Try to use chat template if available\n",
        "    try:\n",
        "        prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    except Exception:\n",
        "        prompt = \"\\n\".join([f\"{m['role'].upper()}: {m['content']}\" for m in messages]) + \"\\nASSISTANT:\"\n",
        "    inputs = tok(prompt, return_tensors=\"pt\").to(mdl.device)\n",
        "    gen_cfg = GenerationConfig(**params)\n",
        "    with torch.no_grad():\n",
        "        out = mdl.generate(**inputs, generation_config=gen_cfg, eos_token_id=tok.eos_token_id)\n",
        "    text = tok.decode(out[0], skip_special_tokens=True)\n",
        "    return text.split(\"ASSISTANT:\", 1)[-1].strip()\n",
        "\n",
        "def ask_llm(user_type: str, question: str, budget_summary: Optional[Dict]) -> str:\n",
        "    system_prompt = build_system_prompt(user_type)\n",
        "    msgs = craft_messages(system_prompt, question, budget_summary)\n",
        "    if RUN_MODE == \"local\":\n",
        "        return generate_local(msgs, GEN_KW.copy())\n",
        "    else:\n",
        "        return generate_hosted(msgs, GEN_KW.copy())\n",
        "\n",
        "# ----------------------------\n",
        "# STREAMLIT UI\n",
        "# ----------------------------\n",
        "st.set_page_config(page_title=\"Personal Finance Chatbot\", page_icon=\"💬\", layout=\"wide\")\n",
        "st.title(\"💬 Personal Finance Chatbot\")\n",
        "st.caption(DISCLAIMER)\n",
        "\n",
        "with st.sidebar:\n",
        "    st.header(\"Settings\")\n",
        "    st.session_state[\"RUN_MODE\"] = st.radio(\"Model backend\", [\"hosted\",\"local\"], index=0,\n",
        "                                            help=\"hosted = Hugging Face Inference API; local = load 4-bit model on GPU\")\n",
        "    RUN_MODE = st.session_state[\"RUN_MODE\"]\n",
        "    st.write(f\"Model: `{MODEL_ID}`\")\n",
        "\n",
        "    st.subheader(\"User Profile\")\n",
        "    user_type = st.radio(\"I am a…\", [\"Student\",\"Professional\"], index=1)\n",
        "    income = st.number_input(\"Monthly income (INR)\", min_value=0, value=50000, step=1000)\n",
        "    goal = st.text_input(\"Main goal (e.g., Save for emergency fund)\")\n",
        "    risk = st.select_slider(\"Risk tolerance\", options=[\"Low\",\"Medium\",\"High\"], value=\"Medium\")\n",
        "\n",
        "    st.subheader(\"Upload Transactions CSV\")\n",
        "    st.markdown(\"Columns required: **date, description, category, amount**\")\n",
        "    file = st.file_uploader(\"Choose CSV\", type=[\"csv\"])\n",
        "\n",
        "# Prepare budget summary (if uploaded)\n",
        "budget_summary = None\n",
        "if file is not None:\n",
        "    try:\n",
        "        df = pd.read_csv(file)\n",
        "        budget_summary = summarize_budget(df)\n",
        "        st.subheader(\"📊 Budget Summary\")\n",
        "        c1, c2, c3 = st.columns(3)\n",
        "        c1.metric(\"Total Inflow (₹)\", f\"{budget_summary['total_inflow']:,.0f}\")\n",
        "        c2.metric(\"Total Outflow (₹)\", f\"{budget_summary['total_outflow']:,.0f}\")\n",
        "        c3.metric(\"Est. Savings (₹)\", f\"{budget_summary['estimated_savings']:,.0f}\")\n",
        "        st.write(\"**Top Categories:**\", budget_summary[\"top_spends\"])\n",
        "        if budget_summary[\"insights\"]:\n",
        "            st.write(\"**Insights:**\")\n",
        "            for tip in budget_summary[\"insights\"]:\n",
        "                st.write(\"•\", tip)\n",
        "    except Exception as e:\n",
        "        st.error(f\"Could not read CSV: {e}\")\n",
        "\n",
        "# Chat section\n",
        "st.subheader(\"Ask a question\")\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state[\"messages\"] = []\n",
        "\n",
        "# Display history\n",
        "for role, content in st.session_state[\"messages\"]:\n",
        "    with st.chat_message(role):\n",
        "        st.markdown(content)\n",
        "\n",
        "# New input\n",
        "prompt = st.chat_input(\"Ask about savings, taxes, or investments…\")\n",
        "if prompt:\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(prompt)\n",
        "    st.session_state[\"messages\"].append((\"user\", prompt))\n",
        "\n",
        "    # Build a richer question including profile\n",
        "    enriched = (\n",
        "        f\"User profile: type={user_type}, income={income}, goal='{goal}', risk={risk}.\\n\"\n",
        "        f\"Task: Answer clearly in INR with step-by-step actions and a 30/50/20-style budget suggestion if relevant.\\n\"\n",
        "        f\"Question: {prompt}\"\n",
        "    )\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            answer = ask_llm(user_type, enriched, budget_summary)\n",
        "            st.markdown(answer or \"No response.\")\n",
        "    st.session_state[\"messages\"].append((\"assistant\", answer or \"\"))\n",
        "\n",
        "st.divider()\n",
        "st.caption(\"Built with Streamlit + IBM Granite (Hugging Face).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OvGsTg2DsyB",
        "outputId": "0083e4ae-3dea-4bb7-9615-28e497cb1d00"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess, threading, time, os\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Start Streamlit\n",
        "def run_streamlit():\n",
        "    cmd = [\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\", \"--server.headless\", \"true\"]\n",
        "    subprocess.run(cmd)\n",
        "\n",
        "# Authenticate and open tunnel\n",
        "ngrok.set_auth_token(os.environ[\"NGROK_AUTHTOKEN\"])\n",
        "public_url = ngrok.connect(8501, \"http\")\n",
        "\n",
        "thread = threading.Thread(target=run_streamlit, daemon=True)\n",
        "thread.start()\n",
        "\n",
        "time.sleep(3)\n",
        "print(\"✅ Public URL:\", public_url)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itMUTNTLDs4p",
        "outputId": "0989f3b6-445e-425b-98d7-17f2562be0b8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Public URL: NgrokTunnel: \"https://6f7e03c554db.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}